{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.utils.data as utils\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/umar_visionx/anaconda3/lib/python3.7/site-packages (21.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.0.3-py3-none-any.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 305 kB/s            \n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.3.1\n",
      "    Uninstalling pip-21.3.1:\n",
      "      Successfully uninstalled pip-21.3.1\n",
      "Successfully installed pip-22.0.3\n",
      "Requirement already satisfied: efficientnet-pytorch in /home/umar_visionx/anaconda3/lib/python3.7/site-packages (0.7.1)\n",
      "Requirement already satisfied: torch in /home/umar_visionx/anaconda3/lib/python3.7/site-packages (from efficientnet-pytorch) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in /home/umar_visionx/anaconda3/lib/python3.7/site-packages (from torch->efficientnet-pytorch) (3.10.0.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/umar_visionx/Backup Plus/Active/Faizan'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path='/media/umar_visionx/Backup Plus/Active/Faizan/image_data/'\n",
    "test_dataset_path='/media/umar_visionx/Backup Plus/Active/Faizan/test_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Admin Note', 'Clinical History And Summary', 'Consult Note', 'Cover Page', 'Imaging Note', 'Insurance Authorization', 'Intake Forms', 'Lab Test', 'Other', 'Patient Profile', 'Prescriptions', 'Referral Letter', 'Requisition Form']\n"
     ]
    }
   ],
   "source": [
    "#getting name of the classes\n",
    "text_path='/media/umar_visionx/Backup Plus/Active/Faizan/doc_classifier_classes.txt'\n",
    "with open(text_path,'r') as file:\n",
    "    classes=file.read().strip().split('\\n')\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #making classes folder in test_data folder\n",
    "# for name in classes:\n",
    "#     path=os.path.join(test_dataset_path,name)\n",
    "#     os.makedirs(path)\n",
    "# print('process completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #copying test data to new folder\n",
    "# test_path='/media/umar_visionx/Backup Plus/Active/Faizan/data/test-set/Active'\n",
    "# for hosp in os.listdir(test_path):\n",
    "#     hosp_path=os.path.join(test_path,hosp)\n",
    "#     for cls in os.listdir(hosp_path):\n",
    "#         if cls in classes:#discarding EOB\n",
    "#             cls_path=os.path.join(hosp_path,cls)\n",
    "#             for folder in os.listdir(cls_path):\n",
    "#                 folder_path=os.path.join(cls_path,folder)\n",
    "#                 for file in os.listdir(folder_path):\n",
    "#                     if file.endswith('.png'):\n",
    "#                         img_path=os.path.join(folder_path,file)\n",
    "#                         dest_path=os.path.join(test_dataset_path,cls)\n",
    "#                         shutil.copy(img_path,dest_path)\n",
    "                        \n",
    "#                     else:\n",
    "#                         pass\n",
    "#         else:\n",
    "#             pass\n",
    "# print(\"copying process completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Admin Note': 0,\n",
       " 'Clinical History And Summary': 1,\n",
       " 'Consult Note': 2,\n",
       " 'Cover Page': 3,\n",
       " 'Imaging Note': 4,\n",
       " 'Insurance Authorization': 5,\n",
       " 'Intake Forms': 6,\n",
       " 'Lab Test': 7,\n",
       " 'Other': 8,\n",
       " 'Patient Profile': 9,\n",
       " 'Prescriptions': 10,\n",
       " 'Referral Letter': 11,\n",
       " 'Requisition Form': 12}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label encoding\n",
    "labl=[]\n",
    "for folder in os.listdir(train_dataset_path):\n",
    "  labl.append(folder)#making the list of folder names which are labels\n",
    "\n",
    "\n",
    "#encoding the labels from the list\n",
    "label_index={}\n",
    "for index,label in enumerate(labl):\n",
    "  label_index[label]=index\n",
    "\n",
    "label_index#label to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Admin Note', 1: 'Clinical History And Summary', 2: 'Consult Note', 3: 'Cover Page', 4: 'Imaging Note', 5: 'Insurance Authorization', 6: 'Intake Forms', 7: 'Lab Test', 8: 'Other', 9: 'Patient Profile', 10: 'Prescriptions', 11: 'Referral Letter', 12: 'Requisition Form'}\n"
     ]
    }
   ],
   "source": [
    "#making dictionary for label decoding\n",
    "index_label={}\n",
    "for key,val in zip(label_index.keys(),label_index.values()):\n",
    "    index_label[val]=key\n",
    "print(index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path):\n",
    "  #getting images path and labels\n",
    "  targets=[]\n",
    "  images=[]\n",
    "  labels=[]\n",
    "  #images_labels=[]\n",
    "  #imges_labels={}\n",
    "  for folder in os.listdir(path):\n",
    "    folder_path=path + folder\n",
    "    for img in os.listdir(folder_path):\n",
    "      img_path=folder_path + '/'+ img\n",
    "      targets.append(folder)\n",
    "      images.append(img_path)\n",
    "\n",
    "  #encoding all labels\n",
    "  for ent in targets:\n",
    "    labels.append(label_index[ent])\n",
    "\n",
    "  #converting into datadrame\n",
    "  dframe= pd.DataFrame()\n",
    "  dframe[\"image_path\"]=images\n",
    "  dframe[\"label\"]=labels\n",
    "  return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_path  label\n",
      "0  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
      "1  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
      "2  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
      "3  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
      "4  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/media/umar_visionx/Backup Plus/Active/Faizan/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/media/umar_visionx/Backup Plus/Active/Faizan/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/media/umar_visionx/Backup Plus/Active/Faizan/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/media/umar_visionx/Backup Plus/Active/Faizan/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/media/umar_visionx/Backup Plus/Active/Faizan/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  label\n",
       "0  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
       "1  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
       "2  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
       "3  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
       "4  /media/umar_visionx/Backup Plus/Active/Faizan/...      0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr=preprocessing(train_dataset_path)\n",
    "df_ts=preprocessing(test_dataset_path)\n",
    "print(df_tr.head())\n",
    "df_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11660\n",
      "1810\n"
     ]
    }
   ],
   "source": [
    "print(len(df_tr))\n",
    "print(len(df_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11664\n"
     ]
    }
   ],
   "source": [
    "# path='/media/umar_visionx/Backup Plus/Active/Faizan/test_data'\n",
    "# total=[]\n",
    "# for fold in os.listdir(path):\n",
    "#     fold_path=os.path.join(path,fold)\n",
    "#     for img in os.listdir(fold_path):\n",
    "#         img_path=os.path.join(fold_path,img)\n",
    "#         total.append(img_path)\n",
    "# print(len(total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv path\n",
    "csv_path='/media/umar_visionx/Backup Plus/Active/Faizan/Visul&TextualFeature_csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving the train and test dataframes\n",
    "#df_tr.to_csv(os.path.join(csv_path,'train_data.csv'),index = False, header=True)\n",
    "#df_ts.to_csv(os.path.join(csv_path,'test_data.csv'),index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_path  label\n",
      "0  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
      "1  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
      "2  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
      "3  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
      "4  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/media/umar_visionx/Backup Plus/Active/Faizan/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/media/umar_visionx/Backup Plus/Active/Faizan/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/media/umar_visionx/Backup Plus/Active/Faizan/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/media/umar_visionx/Backup Plus/Active/Faizan/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/media/umar_visionx/Backup Plus/Active/Faizan/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  label\n",
       "0  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
       "1  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
       "2  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
       "3  /media/umar_visionx/Backup Plus/Active/Faizan/...      0\n",
       "4  /media/umar_visionx/Backup Plus/Active/Faizan/...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dataframe\n",
    "df_train=pd.read_csv(os.path.join(csv_path,'train_data.csv'))\n",
    "df_test=pd.read_csv(os.path.join(csv_path,'test_data.csv'))\n",
    "print(df_train.head())\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =  transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=(224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[123.675, 116.28, 103.53], \n",
    "                             std=[58.395, 57.12, 57.375])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhelixDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        #super().__init__()\n",
    "        self.df = df\n",
    "        self.images = self.df['image_path']\n",
    "        self.labels =self.df['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):       \n",
    "        img_path = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = transform(image)\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "#from PIL import Image\n",
    "# img_path = df_train['image_path'][1]\n",
    "# label = df_train['label'][1]\n",
    "# image = cv2.imread(img_path)\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# image\n",
    "# #image.shape\n",
    "# # #im_pil = Image.fromarray(image)\n",
    "# # image = transform(image)\n",
    "# # image\n",
    "# # #type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          ...,\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008]],\n",
      "\n",
      "         [[-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          ...,\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182]],\n",
      "\n",
      "         [[-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          ...,\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870]]],\n",
      "\n",
      "\n",
      "        [[[-2.1008, -2.1024, -2.1057,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1033, -2.1068,  ..., -2.1010, -2.1008, -2.1008],\n",
      "          [-2.1015, -2.1011, -2.1019,  ..., -2.1008, -2.1008, -2.1014],\n",
      "          ...,\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008]],\n",
      "\n",
      "         [[-2.0182, -2.0199, -2.0232,  ..., -2.0183, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0207, -2.0243,  ..., -2.0185, -2.0182, -2.0182],\n",
      "          [-2.0189, -2.0186, -2.0193,  ..., -2.0182, -2.0182, -2.0188],\n",
      "          ...,\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182]],\n",
      "\n",
      "         [[-1.7870, -1.7887, -1.7920,  ..., -1.7871, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7895, -1.7931,  ..., -1.7873, -1.7870, -1.7870],\n",
      "          [-1.7877, -1.7874, -1.7881,  ..., -1.7870, -1.7870, -1.7876],\n",
      "          ...,\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870]]],\n",
      "\n",
      "\n",
      "        [[[-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          ...,\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008]],\n",
      "\n",
      "         [[-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          ...,\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182]],\n",
      "\n",
      "         [[-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          ...,\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.1016, -2.1051, -2.1049,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1021, -2.1085, -2.1090,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1009, -2.1015, -2.1017,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          ...,\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008]],\n",
      "\n",
      "         [[-2.0190, -2.0226, -2.0225,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0196, -2.0261, -2.0267,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0183, -2.0190, -2.0191,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          ...,\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182]],\n",
      "\n",
      "         [[-1.7878, -1.7914, -1.7913,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7884, -1.7949, -1.7954,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7872, -1.7878, -1.7879,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          ...,\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870]]],\n",
      "\n",
      "\n",
      "        [[[-2.1008, -2.1026, -2.1053,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1031, -2.1058,  ..., -2.1010, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1010, -2.1015,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          ...,\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008]],\n",
      "\n",
      "         [[-2.0182, -2.0201, -2.0229,  ..., -2.0183, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0205, -2.0234,  ..., -2.0185, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0184, -2.0189,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          ...,\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182]],\n",
      "\n",
      "         [[-1.7870, -1.7889, -1.7917,  ..., -1.7871, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7893, -1.7921,  ..., -1.7873, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7872, -1.7877,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          ...,\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870]]],\n",
      "\n",
      "\n",
      "        [[[-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1008,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          ...,\n",
      "          [-2.1008, -2.1008, -2.1015,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1013,  ..., -2.1008, -2.1008, -2.1008],\n",
      "          [-2.1008, -2.1008, -2.1009,  ..., -2.1008, -2.1008, -2.1008]],\n",
      "\n",
      "         [[-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          ...,\n",
      "          [-2.0182, -2.0182, -2.0190,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0188,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          [-2.0182, -2.0182, -2.0183,  ..., -2.0182, -2.0182, -2.0182]],\n",
      "\n",
      "         [[-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          ...,\n",
      "          [-1.7870, -1.7870, -1.7878,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7876,  ..., -1.7870, -1.7870, -1.7870],\n",
      "          [-1.7870, -1.7870, -1.7872,  ..., -1.7870, -1.7870, -1.7870]]]]), tensor([ 2,  0,  2,  1, 11,  5, 11,  2,  3,  1,  5,  3, 12,  1, 12, 11, 12, 12,\n",
      "        11,  1,  0,  2,  8,  3,  4, 11,  8,  8,  8, 11,  7,  4])]\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "train_data = PhelixDataset(df_train)\n",
    "test_data = PhelixDataset(df_test)\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size,shuffle=True,num_workers=3,pin_memory=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size,shuffle=False,num_workers=3,pin_memory=True)\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample)\n",
    "# batch=next(iter(train_dataloader))\n",
    "# print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for img, label in train_dataloader:\n",
    "    print(img.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#efficientnet_b0 = models.efficientnet_b0()\n",
    "#model = EfficientNet.from_name('efficientnet-b0')\n",
    "model = models.efficientnet_b0(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the model layers\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the classifier layer\n",
    "model.classifier = nn.Sequential(nn.Flatten(),\n",
    "                           nn.Linear(1280,512),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(p=0.4),\n",
    "                           nn.Linear(512,128),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(p=0.4),\n",
    "                           nn.Linear(128,13))#totel classes are 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making Classifier layer Trainable                           \n",
    "for parameter in model.classifier.parameters():\n",
    "  parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): ConvNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): ConvNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): ConvNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): ConvNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): ConvNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.4, inplace=False)\n",
       "    (7): Linear(in_features=128, out_features=13, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chkp_path=\"/media/umar_visionx/Backup Plus/Active/Faizan/model_chkpt/model1.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 16.236709635704756\n",
      "Training accuracy: 55.42023849487305\n",
      "-----------\n",
      "Epoch: 1\n",
      "Loss: 14.326438020914793\n",
      "Training accuracy: 60.31732177734375\n",
      "-----------\n",
      "Epoch: 2\n",
      "Loss: 13.135047318413854\n",
      "Training accuracy: 63.35334396362305\n",
      "-----------\n",
      "Epoch: 3\n",
      "Loss: 12.591167248785496\n",
      "Training accuracy: 65.02572631835938\n",
      "-----------\n",
      "Epoch: 4\n",
      "Loss: 11.800845459103584\n",
      "Training accuracy: 67.08404541015625\n",
      "-----------\n",
      "Epoch: 5\n",
      "Loss: 11.252525808289647\n",
      "Training accuracy: 68.7392807006836\n",
      "-----------\n",
      "Epoch: 6\n",
      "Loss: 10.761295653879642\n",
      "Training accuracy: 70.0343017578125\n",
      "-----------\n",
      "Epoch: 7\n",
      "Loss: 10.416289787739515\n",
      "Training accuracy: 71.11492156982422\n",
      "-----------\n",
      "Epoch: 8\n",
      "Loss: 10.236621338874102\n",
      "Training accuracy: 71.42366790771484\n",
      "-----------\n",
      "Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "global_step = 0\n",
    "\n",
    "#put the model in training mode\n",
    "model.train()\n",
    "###\n",
    "train_epoch=[]\n",
    "train_acc=[]\n",
    "val_acc=[]\n",
    "curr_loss=[]\n",
    "#val_epoch=[]\n",
    "#train_losses, valid_losses,valid_acc = [], [], []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  ###\n",
    "  train_epoch.append(epoch)\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  for images, labels in train_dataloader:\n",
    "   \n",
    "      #moving to device\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "  #     # forward pass\n",
    "      logits = model(images)\n",
    "      loss = criterion(logits,labels)\n",
    "    #   print(logits)\n",
    "    #   print(labels)\n",
    "      \n",
    "  \n",
    "      running_loss += loss.item()\n",
    "      predictions = logits.argmax(-1)\n",
    "      correct += (predictions == labels).float().sum()\n",
    "    #   print(loss)\n",
    "  #     # backward pass to get the gradients \n",
    "      loss.backward()\n",
    "\n",
    "  #     # update\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      global_step += 1\n",
    "\n",
    "  \n",
    "  print(\"Loss:\", running_loss / batch_size)\n",
    "  accuracy = 100 * correct / len(df_train)\n",
    "  print(\"Training accuracy:\", accuracy.item())\n",
    "  ###\n",
    "  train_acc.append(accuracy.item())\n",
    "  ###\n",
    "  curr_loss.append(running_loss / batch_size)\n",
    "  #saving model\n",
    "  torch.save(model.state_dict(), model_chkp_path)\n",
    "\n",
    "#   #checking on validation dataset\n",
    "#   #if epoch%5==0:\n",
    "#   model.eval()\n",
    "\n",
    "#   correct = 0\n",
    "#   for batch in val_dataloader:\n",
    "\n",
    "#     #moving to device\n",
    "#     images = images.to(device)\n",
    "#     labels = labels.to(device)\n",
    "\n",
    "\n",
    "#     logits = model(images)\n",
    "\n",
    "#     predictions = logits.argmax(-1)\n",
    "#     correct += (predictions == labels).float().sum()\n",
    "\n",
    "#   accuracy = 100 * correct / len(df_val)\n",
    "#   print(\"Validation accuracy:\", accuracy.item())\n",
    "#     ###\n",
    "#   val_acc.append(accuracy.item())\n",
    "    \n",
    "  print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe from lists to plot the graphs\n",
    "df_an = pd.DataFrame(list(zip(train_epoch, train_acc,curr_loss,val_acc)),\n",
    "               columns =['epoch', 'train_acc','loss','val_acc'])\n",
    "df_an.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting training epochs,loss,accuracy and validation accuracy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "sns.lineplot(data = df_an)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.title(\"loss,train_accuracy,valid_accuracy vrs epochs \")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model\n",
    "model.load_state_dict(torch.load(model_chkp_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking model performance on testset\n",
    "model.eval()\n",
    "labels_output=[]\n",
    "predictions_output=[]\n",
    "\n",
    "###\n",
    "y_pred=[]\n",
    "y_label=[]\n",
    "correct = 0\n",
    "for images,labels in test_dataloader:\n",
    "    \n",
    "    #moving to device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "  \n",
    "    with torch.no_grad():\n",
    "      logits = model(images)\n",
    "\n",
    "\n",
    "    predictions = logits.argmax(-1)\n",
    "    ###\n",
    "    y_pred.append(predictions)\n",
    "    ###\n",
    "    y_label.append(labels)\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "    correct += (predictions == labels).float().sum()\n",
    "    #print(correct)\n",
    "\n",
    "accuracy = 100 * correct / len(df_test)\n",
    "print(\"Testing accuracy:\", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the labels from from tensors and making a list\n",
    "label0=[]\n",
    "label1=[]\n",
    "labels=[] #extracted label\n",
    "\n",
    "for val0 in y_label:\n",
    "  #converted to numpy array\n",
    "  label0.append(val0.cpu().numpy())\n",
    "for val2 in label0:\n",
    "  label1.append(val2.tolist())#converting from array to list of list of labels\n",
    "for val3 in label1:#converting in list of labels\n",
    "  for val4 in val3:\n",
    "    labels.append(val4)\n",
    "\n",
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the predictions from tensors and making a list\n",
    "pred0=[]\n",
    "pred1=[]\n",
    "predictions=[] #extracted label\n",
    "\n",
    "for val0 in y_pred:\n",
    "  #converted to numpy array\n",
    "  pred0.append(val0.cpu().numpy())\n",
    "for val2 in pred0:\n",
    "  pred1.append(val2.tolist())#converting from array to list of list of labels\n",
    "for val3 in pred1:#converting in list of labels\n",
    "  for val4 in val3:\n",
    "    predictions.append(val4)\n",
    "\n",
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#confusion matrix\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = [['Clinical History And Summary','Lab Test','Requisition Form','Other','Admin Note','Patient Profile','Consult Note','Cover Page']], \n",
    "                     columns = [['Clinical History And Summary','Lab Test','Requisition Form','Other','Admin Note','Patient Profile','Consult Note','Cover Page']])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "target_names = ['Clinical History And Summary','Lab Test','Requisition Form','Other','Admin Note','Patient Profile','Consult Note','Cover Page']\n",
    "print(classification_report(labels, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(f\"Accuracy Score: {(metrics.accuracy_score(labels, predictions)) * 100} %\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
