{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch\n",
    "# !pip install transformers\n",
    "#! sudo apt install tesseract-ocr\n",
    "#! pip install pytesseract\n",
    "#! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:10:24.825518: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-04 14:10:24.825686: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Layer#, InputSpec\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "#from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Bidirectional, TimeDistributed, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Admin Note': 0,\n",
       " 'Clinical History And Summary': 1,\n",
       " 'Consult Note': 2,\n",
       " 'Cover Page': 3,\n",
       " 'Imaging Note': 4,\n",
       " 'Insurance Authorization': 5,\n",
       " 'Intake Forms': 6,\n",
       " 'Lab Test': 7,\n",
       " 'Other': 8,\n",
       " 'Patient Profile': 9,\n",
       " 'Prescriptions': 10,\n",
       " 'Referral Letter': 11,\n",
       " 'Requisition Form': 12}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label encoding\n",
    "train_text=\"/media/umar_visionx/Backup Plus/Active/Faizan/textrect_dataset\"\n",
    "labl=[]\n",
    "for folder in os.listdir(train_text):\n",
    "  labl.append(folder)#making the list of folder names which are labels\n",
    "\n",
    "\n",
    "#encoding the labels from the list\n",
    "label_index={}\n",
    "for index,label in enumerate(labl):\n",
    "  label_index[label]=index\n",
    "\n",
    "label_index#label to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max samples in class ( Referral Letter=2363)\n",
      "***********\n",
      "class weights: {'Admin Note': 4.02, 'Clinical History And Summary': 2.26, 'Consult Note': 1.64, 'Cover Page': 1.53, 'Imaging Note': 4.81, 'Insurance Authorization': 2.44, 'Intake Forms': 102.74, 'Lab Test': 3.03, 'Other': 2.41, 'Patient Profile': 32.82, 'Prescriptions': 42.96, 'Referral Letter': 1.0, 'Requisition Form': 1.8}\n",
      "********\n",
      "class/label weights: {0: 4.02, 1: 2.26, 2: 1.64, 3: 1.53, 4: 4.81, 5: 2.44, 6: 102.74, 7: 3.03, 8: 2.41, 9: 32.82, 10: 42.96, 11: 1.0, 12: 1.8}\n"
     ]
    }
   ],
   "source": [
    "#making class weights dictionary (our classes are imbalance)\n",
    "#getting class having maximum samples\n",
    "class_samples={}\n",
    "for folder in os.listdir(train_text):\n",
    "    folder_path=os.path.join(train_text,folder)\n",
    "    class_samples[folder]=len(os.listdir(folder_path))\n",
    "print(f'max samples in class ( {max(class_samples, key=class_samples.get)}={class_samples[max(class_samples, key=class_samples.get)]})')\n",
    "max_samples=int(class_samples[max(class_samples, key=class_samples.get)])\n",
    "print('***********')\n",
    "#getting class weights\n",
    "weights={}\n",
    "for folder in os.listdir(train_text):\n",
    "    folder_path=os.path.join(train_text,folder)\n",
    "    weights[folder]=round(max_samples/int(len(os.listdir(folder_path))),2)\n",
    "print(f'class weights: {weights}')\n",
    "print(\"********\")\n",
    "#changing the names of keys in class weight dictionary\n",
    "class_weight={}\n",
    "for (keys1,values1),(keys2,values2) in zip(weights.items(),label_index.items()):\n",
    "\n",
    "    class_weight[int(values2)]=values1\n",
    "\n",
    "print(f'class/label weights: {class_weight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data='/media/umar_visionx/Backup Plus/Active/Faizan/textrect_saved_vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#loading data from pickle file\n",
    "def ext_data(filepath):\n",
    "\n",
    "    data = []\n",
    "    with open(filepath, 'rb') as fr:\n",
    "        try:\n",
    "            while True:\n",
    "                data.append(pickle.load(fr))\n",
    "        except EOFError:\n",
    "            pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "train_feat_path='/media/umar_visionx/Backup Plus/Active/Faizan/saved_vectors/train_feat.pickle'\n",
    "train_label_path='/media/umar_visionx/Backup Plus/Active/Faizan/saved_vectors/train_label.pickle'\n",
    "test_feat_path='/media/umar_visionx/Backup Plus/Active/Faizan/saved_vectors/test_feat.pickle'\n",
    "test_label_path='/media/umar_visionx/Backup Plus/Active/Faizan/saved_vectors/test_label.pickle'\n",
    "doc_vectors_list_arr=ext_data(train_feat_path)\n",
    "doc_label_list_arr=ext_data(train_label_path)\n",
    "test_vector_list_arr=ext_data(test_feat_path)\n",
    "test_label_list_arr=ext_data(test_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del doc_vectors_list_arr\n",
    "# del doc_label_list_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11660\n",
      "11660\n",
      "512\n",
      "11660\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "#this list have document vectors,every vector,every document have 512 token vectors and every oken vector have 3072 entries\n",
    "print(len(doc_vectors_list_arr))\n",
    "print(len(doc_label_list_arr))\n",
    "print(len(doc_vectors_list_arr[1]))\n",
    "print(len(doc_label_list_arr))\n",
    "print(len(doc_vectors_list_arr[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10494\n",
      "1166\n"
     ]
    }
   ],
   "source": [
    "#splitiing the data into train and validation\n",
    "x_t, x_v, y_t, y_v = train_test_split(doc_vectors_list_arr, doc_label_list_arr, test_size=0.1,random_state=42,stratify=doc_label_list_arr)\n",
    "print(len(x_t))\n",
    "print(len(x_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10494\n",
      "10494\n",
      "1166\n",
      "1166\n",
      "1810\n",
      "1810\n"
     ]
    }
   ],
   "source": [
    "#converting features into numpy array\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "x_val=[]\n",
    "y_val=[]\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "\n",
    "# #train set\n",
    "for ent in x_t:\n",
    "    x_train.append(np.array(ent,dtype=np.float32))\n",
    "\n",
    "for ent in y_t:\n",
    "    y_train.append(np.array(ent,dtype=np.int32))\n",
    "\n",
    "    #validation set\n",
    "for ent in x_v:\n",
    "    x_val.append(np.array(ent,dtype=np.float32))\n",
    "\n",
    "for ent in y_v:\n",
    "     y_val.append(np.array(ent,dtype=np.int32))\n",
    "\n",
    "#test set\n",
    "for ent in test_vector_list_arr:\n",
    "    x_test.append(np.array(ent,dtype=np.float32))\n",
    "\n",
    "for ent in test_label_list_arr:\n",
    "    y_test.append(np.array(ent,dtype=np.int32))\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_val))\n",
    "print(len(y_val))\n",
    "print(len(x_test))\n",
    "print(len(y_test))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del x_train\n",
    "# del y_train\n",
    "# del x_val\n",
    "# del y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U gast==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Hierarchial Attention Layer as described by Hierarchical Attention Networks for Document Classification(2016)\n",
    "    - Yang et. al.\n",
    "    Source: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "    Theano backend\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,attention_dim=768,return_coefficients=False,**kwargs):\n",
    "        # Initializer\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.attention_dim = attention_dim\n",
    "        self.supports_masking = True\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # print(input_shape[0])\n",
    "        # print(input_shape[-1])\n",
    "        # print(input_shape[1])\n",
    "        # Builds all weights\n",
    "        # u = context vector,W = Weight matrix, b = bias vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.W = K.variable(self.init((int(input_shape[-1]), self.attention_dim)),name='W')\n",
    "        \n",
    "        self._trainable_weights = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], int(input_shape[-1])), (input_shape[0], int(input_shape[-1]), 1)]\n",
    "        else:\n",
    "            return input_shape[0], int(input_shape[-1])\n",
    "\n",
    "    # def get_config(self):\n",
    "\n",
    "    #     config = super().get_config().copy()\n",
    "    #     config.update({\n",
    "    #         'attention_dim ': self.attention_dim ,\n",
    "    #         'return_coefficients': self.return_coefficients,\n",
    "    #     })\n",
    "    #     return config\n",
    "\n",
    "    def get_config(self):#to avoiding saving problem.check(https://stackoverflow.com/questions/50837728/valueerror-unknown-layer-capsulelayer and    \n",
    "        #https://stackoverflow.com/questions/58678836/notimplementederror-layers-with-arguments-in-init-must-override-get-conf)\n",
    "        # For serialization with 'custom_objects'\n",
    "        config = super().get_config()\n",
    "        config['attention_dim'] = self.attention_dim\n",
    "        config['return_coefficients'] = self.return_coefficients\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "# embedding_matrix = np.random.random((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "# max_sentenses=1\n",
    "# embedding_layer = Embedding(VOCAB_SIZE,EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH,trainable=False,weights=[embedding_matrix],name='word_embedding')\n",
    "MAX_SEQUENCE_LENGTH=512\n",
    "EMBEDDING_DIM=768\n",
    "# max_sentenses=1\n",
    "\n",
    "# Words  attention\n",
    "emb_input = Input(shape=(MAX_SEQUENCE_LENGTH,EMBEDDING_DIM), dtype='float32',name='embeddings_input')\n",
    "#word_sequences = embedding_layer(word_input)\n",
    "gru_word = Bidirectional(GRU(768, return_sequences=True),name='word_GRU')(emb_input)#by default if we set treturn_sequence=False it will\n",
    "#return only last words hidden state is returm only as the output having vector dimensions=2*GRU units \n",
    "#for bidirectional\n",
    "# word_dense = Dense(1536, activation='relu', name='word_dense')(gru_out) \n",
    "word_attention,word_coff= AttentionLayer(EMBEDDING_DIM,return_coefficients=True,name='word_attention')(gru_word)\n",
    "\n",
    "dense_out = Dense(512,activation='relu', name='word_dense')(word_attention)#we may use different number of units\n",
    "dropout = Dropout(0.5,name='dropout')(dense_out)\n",
    "preds = Dense(13, activation='softmax',name='output')(dropout)\n",
    "model = Model(emb_input, preds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embeddings_input (InputLaye  [(None, 512, 768)]       0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " word_GRU (Bidirectional)    (None, 512, 1536)         7087104   \n",
      "                                                                 \n",
      " word_attention (AttentionLa  [(None, 1536),           1181184   \n",
      " yer)                         (None, 512, 1)]                    \n",
      "                                                                 \n",
      " word_dense (Dense)          (None, 512)               786944    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 13)                6669      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,061,901\n",
      "Trainable params: 9,061,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import math\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, x_train, y_train,batch_size):\n",
    "        self.x, self.y = x_train, y_train\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)#ValueError: axes don't match array\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "        # self.batch_size]\n",
    "        # batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        # self.batch_size]\n",
    "        x_tr = np.array(self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size])#we also added an extra dimension according to our requirement\n",
    "        y_tr = np.array(self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size])\n",
    "\n",
    "\n",
    "        return x_tr,y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=DataGenerator(x_train, y_train,32)\n",
    "val_generator=DataGenerator(x_val, y_val,32)\n",
    "#test_generator = DataGenerator(x_test,y_test,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_tst=np.array(x_test)\n",
    "y_ts=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: dict_keys([5, 2, 1, 7, 0, 11, 8, 3, 4, 12, 10, 9, 6])\n",
      "samples in each label: dict_values([969, 1438, 1045, 780, 588, 2363, 982, 1542, 491, 1312, 55, 72, 23])\n"
     ]
    }
   ],
   "source": [
    "#checking the total samples in each class(label)\n",
    "from collections import Counter\n",
    "print(f\"labels: {Counter(doc_label_list_arr).keys()}\")\n",
    "print(f\"samples in each label: {Counter(doc_label_list_arr).values()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 10:20:41.880122: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 50331648 exceeds 10% of free system memory.\n",
      "2022-04-04 10:20:51.285443: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 50331648 exceeds 10% of free system memory.\n",
      "2022-04-04 10:20:51.360412: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 50331648 exceeds 10% of free system memory.\n",
      "2022-04-04 10:20:51.460537: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 50331648 exceeds 10% of free system memory.\n",
      "2022-04-04 10:20:51.537034: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 50331648 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 1510s 5s/step - loss: 0.6889 - acc: 0.8885 - val_loss: 0.5058 - val_acc: 0.8662\n",
      "328/328 [==============================] - 1509s 5s/step - loss: 0.6767 - acc: 0.8906 - val_loss: 0.5493 - val_acc: 0.8473\n",
      "328/328 [==============================] - 1522s 5s/step - loss: 0.6148 - acc: 0.8902 - val_loss: 0.4619 - val_acc: 0.8799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 11:36:28.174565: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/umar_visionx/Backup Plus/Active/Faizan/word_att_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/umar_visionx/Backup Plus/Active/Faizan/word_att_model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f2418385df0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f243fdd1d00> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 1514s 5s/step - loss: 0.7217 - acc: 0.8816 - val_loss: 0.5602 - val_acc: 0.8662\n",
      "328/328 [==============================] - 1516s 5s/step - loss: 1.2225 - acc: 0.8364 - val_loss: 0.5596 - val_acc: 0.8551\n",
      "328/328 [==============================] - 1516s 5s/step - loss: 0.9879 - acc: 0.8504 - val_loss: 0.6374 - val_acc: 0.8139\n",
      "290/328 [=========================>....] - ETA: 2:49 - loss: 0.9638 - acc: 0.8411"
     ]
    }
   ],
   "source": [
    "#generator=training_generator\n",
    "#from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "#batch_size = 32\n",
    "checkpoint_path='/media/umar_visionx/Backup Plus/Active/Faizan/word_att_model'\n",
    "#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)#early stopping\n",
    "#mc = ModelCheckpoint(checkpoint_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)#model checkpoint\n",
    "loss_list=[]\n",
    "#loss_list.append(history.history['val_loss'][0])\n",
    "history=model.fit(train_generator,validation_data=val_generator, epochs=1,class_weight=class_weight,shuffle=True)\n",
    "loss_list.append(history.history['val_loss'][0])\n",
    "for i in range(20):\n",
    "    history=model.fit(train_generator,validation_data=val_generator, epochs=1,class_weight=class_weight,shuffle=True)\n",
    "    loss=history.history['val_loss'][0]\n",
    "    if loss<loss_list[-1]:\n",
    "        #saving the model\n",
    "        model.save(checkpoint_path)\n",
    "        loss_list.append(history.history['val_loss'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_7_layer_call_fn, gru_cell_7_layer_call_and_return_conditional_losses, gru_cell_8_layer_call_fn, gru_cell_8_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/umar_visionx/Backup Plus/Active/Faizan/word_att_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/umar_visionx/Backup Plus/Active/Faizan/word_att_model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7fc2b02493a0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7fc1e01c3370> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# #saving the model\n",
    "# model.save('/media/umar_visionx/Backup Plus/Active/Faizan/word_att_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:20:00.155311: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-04 14:20:00.179663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-04 14:20:00.183929: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-04 14:20:00.254583: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-04 14:20:00.272795: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-04 14:20:00.273910: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-04 14:20:00.290046: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-04 14:20:00.600940: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-04 14:20:00.909269: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embeddings_input (InputLaye  [(None, 512, 768)]       0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " word_GRU (Bidirectional)    (None, 512, 1536)         7087104   \n",
      "                                                                 \n",
      " word_attention (AttentionLa  [(None, 1536),           1181184   \n",
      " yer)                         (None, 512, 1)]                    \n",
      "                                                                 \n",
      " word_dense (Dense)          (None, 512)               786944    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 13)                6669      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,061,901\n",
      "Trainable params: 9,061,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('/media/umar_visionx/Backup Plus/Active/Faizan/word_att_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.09565596282482147],\n",
       " 'acc': [0.9690299034118652],\n",
       " 'val_loss': [0.44553402066230774],\n",
       " 'val_acc': [0.8936535120010376]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:20:13.932218: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2846883840 exceeds 10% of free system memory.\n",
      "2022-04-04 14:20:18.836347: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 50331648 exceeds 10% of free system memory.\n",
      "2022-04-04 14:20:18.836425: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 50331648 exceeds 10% of free system memory.\n",
      "2022-04-04 14:20:18.836699: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 50331648 exceeds 10% of free system memory.\n",
      "2022-04-04 14:20:18.836791: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 50331648 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 88s 1s/step - loss: 1.6216 - acc: 0.6845\n",
      "Test loss: 1.6215522289276123\n",
      "Test accuracy: 0.6845303773880005\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_tst, y_ts, verbose = 1) \n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ts[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99852657e-01, 3.51574217e-20, 6.65618336e-06, 1.42585204e-05,\n",
       "        4.12751638e-16, 1.62662474e-07, 0.00000000e+00, 4.11187777e-17,\n",
       "        4.24932978e-08, 1.11167533e-20, 1.03712668e-31, 1.07694934e-04,\n",
       "        1.85271183e-05],\n",
       "       [9.76409435e-01, 2.12820221e-17, 9.37808945e-05, 4.39146424e-06,\n",
       "        4.55390066e-12, 1.04569726e-05, 0.00000000e+00, 1.56020121e-13,\n",
       "        3.63488617e-08, 3.30945011e-19, 3.74633896e-26, 2.04123762e-02,\n",
       "        3.06940940e-03],\n",
       "       [9.99008596e-01, 5.26150362e-20, 1.60840250e-06, 9.80917597e-04,\n",
       "        2.54986934e-16, 5.32482645e-08, 0.00000000e+00, 3.04127195e-19,\n",
       "        1.02670685e-06, 7.93316234e-24, 8.28183892e-34, 7.74346336e-06,\n",
       "        1.47977097e-08],\n",
       "       [9.97114301e-01, 3.89703236e-16, 2.80252698e-05, 2.78629502e-03,\n",
       "        3.31891876e-13, 1.41173791e-06, 5.19026019e-37, 1.37459450e-15,\n",
       "        1.58872790e-05, 8.65341685e-20, 6.27584032e-27, 5.37528940e-05,\n",
       "        2.60057476e-07]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0 ... 12 12 12]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = np.argmax(predictions, axis = 1)\n",
    "\n",
    "print(pred) \n",
    "#print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.64      0.61        83\n",
      "           1       0.60      0.69      0.64       336\n",
      "           2       0.57      0.72      0.64       369\n",
      "           3       0.92      0.93      0.93       179\n",
      "           4       0.72      0.60      0.66        63\n",
      "           5       0.36      0.26      0.30        31\n",
      "           6       0.00      0.00      0.00         8\n",
      "           7       0.89      0.83      0.86       181\n",
      "           8       0.76      0.32      0.45       266\n",
      "           9       0.63      0.76      0.69        41\n",
      "          10       1.00      0.22      0.36         9\n",
      "          11       0.68      0.89      0.77       169\n",
      "          12       0.93      0.75      0.83        75\n",
      "\n",
      "    accuracy                           0.68      1810\n",
      "   macro avg       0.67      0.59      0.60      1810\n",
      "weighted avg       0.70      0.68      0.67      1810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_ts,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6845303867403315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "229bb8c562dc6514b703716efac498ecdfe3964b4336041201b73c82b78cece5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
