{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch\n",
    "# !pip install transformers\n",
    "#! sudo apt install tesseract-ocr\n",
    "#! pip install pytesseract\n",
    "#! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "# tqdm.tqdm(dirs, desc='dirs') \n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer #tokanization and removel of punctuation \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Layer#, InputSpec\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "#from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Bidirectional, TimeDistributed, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Admin Note', 'Clinical History And Summary', 'Consult Note', 'Cover Page', 'Imaging Note', 'Insurance Authorization', 'Intake Forms', 'Lab Test', 'Other', 'Patient Profile', 'Prescriptions', 'Referral Letter', 'Requisition Form']\n"
     ]
    }
   ],
   "source": [
    "#classes\n",
    "txt_path='/media/umar_visionx/Backup Plus/Active/Faizan/doc_classifier_classes.txt'\n",
    "with open(txt_path,'r') as file:\n",
    "    classes=file.read().split('\\n')\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to extract text from the document we are applying OCR to each document\n",
    "def apply_ocr(sample):\n",
    "\n",
    "        # apply ocr to the image\n",
    "        #Extracting words and bounding boxes from images\n",
    "        ocr_dframe = pytesseract.image_to_data(sample, output_type='data.frame')\n",
    "        #return a subset of the DataFrame's columns based on the column dtypes.\n",
    "        float_cols = ocr_dframe.select_dtypes('float').columns\n",
    "        #dropping the rows where atleast one element is not present(i.e \"text\" is not present)\n",
    "        ocr_dframe = ocr_dframe.dropna().reset_index(drop=True)\n",
    "        #selecting float_cols,round along zero,changing type to integer\n",
    "        ocr_dframe[float_cols] = ocr_dframe[float_cols].round(0).astype(int)\n",
    "        #replace field that's entirely space (or empty) with NaN check https://stackoverflow.com/questions/13445241/replacing-blank-values-white-space-with-nan-in-pandas\n",
    "        ocr_dframe = ocr_dframe.replace(r'^\\s*$', np.nan, regex=True)\n",
    "        #dropping the rows where atleast one element is not present(i.e \"text\" is not present)\n",
    "        ocr_dframe = ocr_dframe.dropna().reset_index(drop=True)\n",
    "\n",
    "        # get the words and actual (unnormalized) bounding boxes\n",
    "        #words = [word for word in ocr_dframe.text if str(word) != 'nan'])\n",
    "        words = list(ocr_dframe.text)#making the list of column \"text\" in 'ocr_dframe' dataframe\n",
    "        #words = [str(w) for w in words] #converting into list of string of words from the last words list \n",
    "\n",
    "        #making a list of words in the image\n",
    "        image_words=[]\n",
    "        for word in words:\n",
    "          if str(word)!=\"nan\":\n",
    "            image_words.append(str(word))\n",
    "\n",
    "      \n",
    "        #joining words with space\n",
    "        words= \" \".join(image_words)\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making folders and saving the text files (with extracted text from image) in related folders\n",
    "def extract_text(source_path,dest_path):\n",
    "    #first making folders\n",
    "    try:\n",
    "        os.makedirs(dest_path)\n",
    "        for ent in classes:\n",
    "            folder_path=os.path.join(dest_path,ent)\n",
    "            os.makedirs(folder_path)\n",
    "            print(\"Folder making process completed\")\n",
    "    except:\n",
    "        pass\n",
    "    for folder in os.listdir(source_path):\n",
    "        folder_path=os.path.join(source_path,folder)\n",
    "        for files in os.listdir(folder_path):\n",
    "            f_path=os.path.join(folder_path,files)#path of image\n",
    "            image = Image.open(f_path)\n",
    "            image = image.convert(\"RGB\")\n",
    "            para=apply_ocr(image) #getting words from image\n",
    "        \n",
    "        #making directory to save the files with same file and folder name\n",
    "            txt_file=os.path.splitext(files)[0] + \".txt\" #making same name of file as image\n",
    "            txt_path=os.path.join(dest_path,os.path.join(folder,txt_file))\n",
    "        #writing and saving file\n",
    "            with open(txt_path, \"w\") as file:\n",
    "                file.write(para)\n",
    "    print(\"All text files are created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting text from images and creating text files \n",
    "image_data_source=\"/media/umar_visionx/Backup Plus/Active/Faizan/image_data\"#image data source path\n",
    "test_image_source=\"/media/umar_visionx/Backup Plus/Active/Faizan/test_images\"#test image data source path\n",
    "text_dest=\"/media/umar_visionx/Backup Plus/Active/Faizan/dataset\"#extracted text files destinatio path for image data\n",
    "test_text_dest=\"/media/umar_visionx/Backup Plus/Active/Faizan/test_dataset\"#extracted text files destination path from test image data\n",
    "\n",
    "extract_text(image_data_source,text_dest)\n",
    "extract_text(test_image_source,test_text_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing special character and numbers\n",
    "def rem_sp_char(words):\n",
    "    pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]'  # defined pattern to keep\n",
    "    words=re.sub(pattern, '', words)\n",
    "    return words\n",
    "\n",
    "#tokanizing and removing punctuations\n",
    "def remove_punct(words):\n",
    "  tokenizer = RegexpTokenizer(r'\\w+')\n",
    "  tokens=tokenizer.tokenize(words)\n",
    "  return tokens\n",
    "\n",
    "#lemmitization\n",
    "def lemmitizar(words):\n",
    "  lemmatized_words=[]\n",
    "  l = WordNetLemmatizer()\n",
    "  for word in words:\n",
    "      token = l.lemmatize(word)\n",
    "      lemmatized_words.append(token)\n",
    "  return lemmatized_words\n",
    "\n",
    "#removing stop-words\n",
    "def remove_stopwords(words):\n",
    "  filter_words = []\n",
    "  Stopwords = stopwords.words('english')\n",
    "  for word in words:\n",
    "      if word not in Stopwords:\n",
    "          filter_words.append(word)\n",
    "  return filter_words\n",
    "\n",
    "#converting to lowercase\n",
    "def lower_case(words):\n",
    "  lower_words=[]\n",
    "  for word in words:\n",
    "    lower_words.append(word.lower())\n",
    "  return lower_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of text files\n",
    "data_path='/media/umar_visionx/Backup Plus/Active/Faizan/dataset'\n",
    "test_path='/media/umar_visionx/Backup Plus/Active/Faizan/test_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Admin Note': 0,\n",
       " 'Clinical History And Summary': 1,\n",
       " 'Consult Note': 2,\n",
       " 'Cover Page': 3,\n",
       " 'Imaging Note': 4,\n",
       " 'Insurance Authorization': 5,\n",
       " 'Intake Forms': 6,\n",
       " 'Lab Test': 7,\n",
       " 'Other': 8,\n",
       " 'Patient Profile': 9,\n",
       " 'Prescriptions': 10,\n",
       " 'Referral Letter': 11,\n",
       " 'Requisition Form': 12}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label encoding\n",
    "labl=[]\n",
    "for folder in os.listdir(data_path):\n",
    "  labl.append(folder)#making the list of folder names which are labels\n",
    "\n",
    "\n",
    "#encoding the labels from the list\n",
    "label_index={}\n",
    "for index,label in enumerate(labl):\n",
    "  label_index[label]=index\n",
    "\n",
    "label_index#label to index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dictionaries and lists of text and labels\n",
    "def preprocessed_df(data_path):\n",
    "    data_dict={}\n",
    "    #txt_file_path=[]\n",
    "    labels=[]\n",
    "    text=[]\n",
    "    encoded_labels=[]\n",
    "    for clss in os.listdir(data_path):\n",
    "        clss_path=os.path.join(data_path,clss)\n",
    "        for txt_file in os.listdir(clss_path):\n",
    "            file_path=os.path.join(clss_path,txt_file)\n",
    "            #reading text file\n",
    "            with open(file_path, \"r\") as file:\n",
    "                para = file.read().rstrip('\\n')#rstrip method removes trailing character based on string argument passed\n",
    "\n",
    "            #apply preprocessing functions on para\n",
    "                words = rem_sp_char(para) #removing special character and numbers\n",
    "                tokens = remove_punct(words) #removing punctuations and tokanization\n",
    "                tokens = lemmitizar(tokens) #lemmitization\n",
    "                tokens = remove_stopwords(tokens) ##removing stop-words\n",
    "                tokens = lower_case(tokens) #converting to lowercase\n",
    "\n",
    "            #joining words with space and removing single tetter tokens\n",
    "                parag=' '.join( [word for word in tokens if len(word)>1] )\n",
    "            #parag=' '.join( [word for word in tokens if len(word)>1 and word.isalpha()] )#if we dont want numeric values\n",
    "\n",
    "\n",
    "            #inseting data into lists and dictionaries\n",
    "                text.append(parag)\n",
    "            #text_file_path.append(file_path)\n",
    "                labels.append(clss)\n",
    "                data_dict[parag]=clss\n",
    "#label encoding\n",
    "    for ent in labels:\n",
    "        encoded_labels.append(label_index[ent])\n",
    "    \n",
    "\n",
    "\n",
    "    df=pd.DataFrame()\n",
    "    df['text']=text\n",
    "    df['labels']=encoded_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29 2021 10 42 alberta health services rrduser ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30 2021 09 34 alberta health services rrduser ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29 2021 11 03 alberta health services rrduser ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021 08 25 alberta health services rrduser 403...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahs 2021 23 05 am page alberta health services...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  29 2021 10 42 alberta health services rrduser ...       0\n",
       "1  30 2021 09 34 alberta health services rrduser ...       0\n",
       "2  29 2021 11 03 alberta health services rrduser ...       0\n",
       "3  2021 08 25 alberta health services rrduser 403...       0\n",
       "4  ahs 2021 23 05 am page alberta health services...       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=preprocessed_df(data_path)\n",
    "df_test=preprocessed_df(test_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11660\n",
      "1810\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#checking null values in dataframes\n",
    "print(df.isnull().any().sum())\n",
    "print(df_test.isnull().any().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels after Encoding:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique Labels after Encoding:  {df.labels.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#BERT base model\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True,)#model returns all hidden-states.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(df)\n",
    "#val_dataset = Dataset.from_pandas(df_val)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_example(sample,max_seq_length=512):\n",
    "    words=sample['text']\n",
    "    label=sample['labels']\n",
    "    #splitting all words of sentence in to into tokens\n",
    "    tokenized_text = tokenizer.tokenize(words)\n",
    "    # Truncation of token_boxes\n",
    "    special_tokens_count = 2 \n",
    "    if len(tokenized_text) > max_seq_length - special_tokens_count:\n",
    "      tokenized_text = tokenized_text[: (max_seq_length - special_tokens_count)]#to make place available for cls and sep tokens\n",
    "    # Adding CLS and SEP tokens\n",
    "    input_sequence = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "    # Map the tokens to their vocabulary indeces\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "    #padding length\n",
    "    padding_length = max_seq_length - len(input_sequence)\n",
    "    indexed_tokens += [0] * padding_length#zero padding to maximum length\n",
    "    #as we are giving as a single sentence so mark each token as belonging to sentence 1\n",
    "    segments_ids = [1] * max_seq_length\n",
    "    pad_masks = [1] * len(input_sequence) + [0] * padding_length\n",
    "\n",
    "\n",
    "    assert len(indexed_tokens) == max_seq_length\n",
    "    assert len(segments_ids) ==max_seq_length\n",
    "    assert len(pad_masks) == max_seq_length\n",
    "\n",
    "    #converting into pytorch tensors\n",
    "    tokens_tensor = torch.tensor(indexed_tokens)\n",
    "    segments_tensors = torch.tensor(segments_ids)\n",
    "    pad_mask_tensors=torch.tensor(pad_masks)\n",
    "    label_tensors=torch.tensor(label)\n",
    "    \n",
    "  \n",
    "    tok_tensors.append(tokens_tensor)\n",
    "    seg_tensors.append(segments_tensors)\n",
    "    lab_tensors.append(label_tensors)\n",
    "    pad_msk_tensors.append(pad_mask_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_df(dataset):\n",
    "    global tok_tensors,seg_tensors,lab_tensors,pad_msk_tensors\n",
    "\n",
    "    tok_tensors=[]\n",
    "    seg_tensors=[]\n",
    "    lab_tensors=[]\n",
    "    pad_msk_tensors=[]\n",
    "    dataset.map(encode_example)\n",
    "    # df=pd.DataFrame()\n",
    "    # df['token_tensors']=tok_tensors\n",
    "    # df['segment_tensor']=seg_tensors\n",
    "    # df['labels']=lab_tensors\n",
    "    return tok_tensors,seg_tensors,lab_tensors,pad_msk_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70061d9d50ef4723a6d5d068883513fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11660 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabadb0e152e4dd2868dfc797dc86508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1810 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_token_tensor,tr_segment_tensors,tr_label_tensors,tr_pad_msk_tensors=iter_df(train_dataset)\n",
    "ts_token_tensor,ts_segment_tensors,ts_label_tensorsts,ts_pad_msk_tensors=iter_df(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "class PhelixDataset(Dataset):\n",
    "    def __init__(self, token_tens,label_tens,seg_tens,pd_msk_tens):\n",
    "        #super().__init__()\n",
    "        self.token_tens = token_tens\n",
    "        self.seg_tens=seg_tens\n",
    "        self.label_tens=label_tens\n",
    "        self.pd_msk_tens=pd_msk_tens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_tens)\n",
    "    \n",
    "    def __getitem__(self, index):       \n",
    "        tokens_id = self.token_tens[index]\n",
    "        segment_id = self.seg_tens[index]\n",
    "        label=self.label_tens[index]\n",
    "        pad_mask=self.pd_msk_tens[index]\n",
    "\n",
    "        return tokens_id,label,segment_id,pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101,  2403, 25682,  5641,  2654,  2484, 22851,  2102,  2000, 28754,\n",
      "          2683, 16932, 17788,  2575,  2487,  3931,  2013,  2092,  2740,  6786,\n",
      "          6904,  2595,  3438, 26224, 16932, 17788,  2575,  2487,  2092,  2740,\n",
      "          5624,  9697,  2966,  9349,  5292, 17414,  6523,  7941,  6098,  9004,\n",
      "          2906,  3802,  2015,  1051,  2063,  2000, 18856, 19429,  4315, 16216,\n",
      "          8840,  2140,  5776,  2171,  2966,  9349,  4205,  3231, 21650,  2184,\n",
      "          2760,  2238,  6122,  6098,  4751,  2238,  2403, 25682,  2260,  4002,\n",
      "         15017, 12740,  5179, 13642, 10550,  9948,  4647,  1058,  2509,  2015,\n",
      "          1019,  2243,  2581,  2710,  6523,  7941,  5292, 18235,  2078,  2363,\n",
      "          2238,  6400,  6523,  7941,  5292, 17414,  2023,  6523,  7941, 11333,\n",
      "         13012, 18655,  6887, 20806,  2595,  9932,  4748, 10020,  3353,  9349,\n",
      "          3942,  7479,  6887, 20806,  2595,  9932,  4553,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), tensor([0]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "train_data = PhelixDataset(tr_token_tensor,tr_label_tensors,tr_segment_tensors,tr_pad_msk_tensors)\n",
    "test_data = PhelixDataset(ts_token_tensor,ts_label_tensorsts,ts_segment_tensors,ts_pad_msk_tensors)\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size,shuffle=True,num_workers=3,pin_memory=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = batch_size,shuffle=False,num_workers=3,pin_memory=True)\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample)\n",
    "# batch=next(iter(train_dataloader))\n",
    "# print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "for token, label,seg,pad_mask in train_dataloader:\n",
    "    print(token.shape)\n",
    "    print(label.shape)\n",
    "    print(seg.shape)\n",
    "    print(pad_mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training the model\n",
    "# global_step = 0\n",
    "\n",
    "# #put the model in training mode\n",
    "# model.train()\n",
    "# ###\n",
    "# train_epoch=[]\n",
    "# train_acc=[]\n",
    "# val_acc=[]\n",
    "# curr_loss=[]\n",
    "# #val_epoch=[]\n",
    "# #train_losses, valid_losses,valid_acc = [], [], []\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#   print(\"Epoch:\", epoch)\n",
    "#   ###\n",
    "#   train_epoch.append(epoch)\n",
    "#   running_loss = 0.0\n",
    "#   correct = 0\n",
    "#   for images, labels in train_dataloader:\n",
    "   \n",
    "#       #moving to device\n",
    "#       images = images.to(device)\n",
    "#       labels = labels.to(device)\n",
    "\n",
    "#   #     # forward pass\n",
    "#       logits = model(images)\n",
    "#       loss = criterion(logits,labels)\n",
    "#     #   print(logits)\n",
    "#     #   print(labels)\n",
    "      \n",
    "  \n",
    "#       running_loss += loss.item()\n",
    "#       predictions = logits.argmax(-1)\n",
    "#       correct += (predictions == labels).float().sum()\n",
    "#     #   print(loss)\n",
    "#   #     # backward pass to get the gradients \n",
    "#       loss.backward()\n",
    "\n",
    "#   #     # update\n",
    "#       optimizer.step()\n",
    "#       optimizer.zero_grad()\n",
    "#       global_step += 1\n",
    "\n",
    "  \n",
    "#   print(\"Loss:\", running_loss / batch_size)\n",
    "#   accuracy = 100 * correct / len(df_train)\n",
    "#   print(\"Training accuracy:\", accuracy.item())\n",
    "#   ###\n",
    "#   train_acc.append(accuracy.item())\n",
    "#   ###\n",
    "#   curr_loss.append(running_loss / batch_size)\n",
    "#   #saving model\n",
    "#   torch.save(model.state_dict(), model_chkp_path)\n",
    "\n",
    "# #   #checking on validation dataset\n",
    "# #   #if epoch%5==0:\n",
    "# #   model.eval()\n",
    "\n",
    "# #   correct = 0\n",
    "# #   for batch in val_dataloader:\n",
    "\n",
    "# #     #moving to device\n",
    "# #     images = images.to(device)\n",
    "# #     labels = labels.to(device)\n",
    "\n",
    "\n",
    "# #     logits = model(images)\n",
    "\n",
    "# #     predictions = logits.argmax(-1)\n",
    "# #     correct += (predictions == labels).float().sum()\n",
    "\n",
    "# #   accuracy = 100 * correct / len(df_val)\n",
    "# #   print(\"Validation accuracy:\", accuracy.item())\n",
    "# #     ###\n",
    "# #   val_acc.append(accuracy.item())\n",
    "    \n",
    "#   print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 15 11:14:06 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0  On |                  N/A |\n",
      "|  0%   38C    P2    68W / 320W |   1417MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1615      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    0   N/A  N/A      5402      G   /usr/lib/xorg/Xorg                 89MiB |\n",
      "|    0   N/A  N/A      6074      G   /usr/bin/gnome-shell               21MiB |\n",
      "|    0   N/A  N/A      6758      G   /opt/freedownloadmanager/fdm        2MiB |\n",
      "|    0   N/A  N/A      6967      G                                      20MiB |\n",
      "|    0   N/A  N/A    134544      C   ...nda3/envs/lmv2/bin/python     1231MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#testing GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data='/media/umar_visionx/Backup Plus/Active/Faizan/saved_vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def embed(data_loader,train):\n",
    "  #creating word embeddings\n",
    "  model.eval()\n",
    "  # token_labels_list=[]\n",
    "  # concat_token_vect=[]#define list to append concatinated token vectors\n",
    "  # doc_vectors_list=[]\n",
    "  # doc_label_list=[]\n",
    "  #to save data as arrays \n",
    "  token_labels_list_arr=[]\n",
    "  concat_token_vect_arr=[]\n",
    "  # doc_vectors_list_arr=[]\n",
    "  # doc_label_list_arr=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  for tokens,label,seg_id,_ in tqdm(data_loader):\n",
    "      if train==True:\n",
    "        ft=open(os.path.join(saved_data,'train_feat.pickle'), 'ab')\n",
    "        lb=open(os.path.join(saved_data,'train_label.pickle'), 'ab')\n",
    "      else:\n",
    "        ft=open(os.path.join(saved_data,'test_feat.pickle'), 'ab')\n",
    "        lb=open(os.path.join(saved_data,'test_label.pickle'), 'ab')\n",
    "\n",
    "      #moving to device\n",
    "      label=int(label)\n",
    "      tokens=tokens.to(device)\n",
    "      seg_id=seg_id.to(device)\n",
    "      with torch.no_grad():\n",
    "        output = model(tokens,seg_id)\n",
    "      #we set output_hidden_state=True in our pre trained model so, we will get the hidden states from all 13 layers at third positiob\n",
    "      hidden_states = output[2]\n",
    "      #For futher info please check https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "      # Concatenating tensors for all layers and creating new dim with the help of stack\n",
    "      token_embed = torch.stack(hidden_states, dim=0)#token embeddings(torch.Size([13, 1, 512, 768]))\n",
    "      token_embed = torch.squeeze(token_embed, dim=1)#removing batch dimension(torch.Size([13, 512, 768]))\n",
    "      token_embed = token_embed.permute(1,0,2)# swapping dimensions(torch.Size([512, 13, 768]))\n",
    "      #iterating for each token in whole document/sentense\n",
    "      to_append_vect=[]\n",
    "      to_append_vect_arr=[]\n",
    "      #to_append_label=[]\n",
    "      for token in token_embed:\n",
    "          #token vector of size 13*768 where 13 are number of layers while we want to get only last 4 layers i.e 4*768\n",
    "          #after concatination last four layers vector dimensions will be 4*768=3,072(singlre word vector length i.e per token)\n",
    "          #concatinating vectors\n",
    "          add1 = torch.add(token[-1], token[-2])\n",
    "          add2=torch.add(token[-3], token[-4])\n",
    "          full_vector=torch.add(add1,add2)\n",
    "          #concatcat_vector = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "          # concat_token_vect.append(full_vector)\n",
    "          #we have append 512 vectors of size 768 for each document\n",
    "          # token_labels_list.append(label)\n",
    "          token_labels_list_arr.append(label)\n",
    "          concat_token_vect_arr.append(full_vector.cpu().numpy())\n",
    "\n",
    "          to_append_vect_arr.append(full_vector.cpu().numpy())\n",
    "          to_append_vect.append(full_vector)\n",
    "          #to_append_label.append(label)\n",
    "      # print(len(to_append_vect_arr))\n",
    "      # print(len(to_append_vect))\n",
    "\n",
    "\n",
    "      # doc_vectors_list.append(to_append_vect)\n",
    "      # doc_label_list.append(label)\n",
    "      # doc_vectors_list_arr.append(to_append_vect_arr)\n",
    "      # doc_label_list_arr.append(label)\n",
    "\n",
    "      # with open(os.path.join(saved_data,\"train_features.pickle\"),'ab') as f:\n",
    "      #   pickle.dump(to_append_vect_arr,f)\n",
    "      # with open(os.path.join(saved_data,\"train_labels.pickle\"),'ab') as f:\n",
    "      #   pickle.dump(label,f)\n",
    "      pickle.dump(to_append_vect_arr,ft)\n",
    "      pickle.dump(label,lb)\n",
    "  ft.close()\n",
    "  lb.close()\n",
    "\n",
    "  #return doc_vectors_list_arr,doc_label_list_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Embeddings\n",
    "\n",
    "embed(train_dataloader,True)\n",
    "embed(test_dataloader,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#loading data from pickle file\n",
    "def ext_data(filepath):\n",
    "\n",
    "    data = []\n",
    "    with open(filepath, 'rb') as fr:\n",
    "        try:\n",
    "            while True:\n",
    "                data.append(pickle.load(fr))\n",
    "        except EOFError:\n",
    "            pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "train_feat_path='/media/umar_visionx/Backup Plus/Active/Faizan/saved_vectors/train_feat.pickle'\n",
    "train_label_path='/media/umar_visionx/Backup Plus/Active/Faizan/saved_vectors/train_label.pickle'\n",
    "test_feat_path='/media/umar_visionx/Backup Plus/Active/Faizan/saved_vectors/test_feat.pickle'\n",
    "test_label_path='/media/umar_visionx/Backup Plus/Active/Faizan/saved_vectors/test_label.pickle'\n",
    "doc_vectors_list_arr=ext_data(train_feat_path)\n",
    "doc_label_list_arr=ext_data(train_label_path)\n",
    "test_vector_list_arr=ext_data(test_feat_path)\n",
    "test_label_list_arr=ext_data(test_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del doc_vectors_list_arr\n",
    "# del doc_label_list_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11660\n",
      "11660\n",
      "512\n",
      "11660\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "#this list have document vectors,every vector,every document have 512 token vectors and every oken vector have 3072 entries\n",
    "print(len(doc_vectors_list_arr))\n",
    "print(len(doc_label_list_arr))\n",
    "print(len(doc_vectors_list_arr[1]))\n",
    "print(len(doc_label_list_arr))\n",
    "print(len(doc_vectors_list_arr[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10494\n",
      "1166\n"
     ]
    }
   ],
   "source": [
    "#splitiing the data into train and validation\n",
    "x_t, x_v, y_t, y_v = train_test_split(doc_vectors_list_arr, doc_label_list_arr, test_size=0.1,random_state=42,stratify=doc_label_list_arr)\n",
    "print(len(x_t))\n",
    "print(len(x_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1810\n",
      "1810\n"
     ]
    }
   ],
   "source": [
    "#converting features into numpy array\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "x_val=[]\n",
    "y_val=[]\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "\n",
    "# #train set\n",
    "# for ent in x_t:\n",
    "#     x_train.append(np.array(ent,dtype=np.float32))\n",
    "\n",
    "# for ent in y_t:\n",
    "#     y_train.append(np.array(ent,dtype=np.int32))\n",
    "\n",
    "#     #validation set\n",
    "# for ent in x_v:\n",
    "#     x_val.append(np.array(ent,dtype=np.float32))\n",
    "\n",
    "# for ent in y_v:\n",
    "#      y_val.append(np.array(ent,dtype=np.int32))\n",
    "\n",
    "#test set\n",
    "for ent in test_vector_list_arr:\n",
    "    x_test.append(np.array(ent,dtype=np.float32))\n",
    "\n",
    "for ent in test_label_list_arr:\n",
    "    y_test.append(np.array(ent,dtype=np.int32))\n",
    "\n",
    "# print(len(x_train))\n",
    "# print(len(y_train))\n",
    "# print(len(x_val))\n",
    "# print(len(y_val))\n",
    "print(len(x_test))\n",
    "print(len(y_test))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del x_train\n",
    "# del y_train\n",
    "# del x_val\n",
    "# del y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For more info about attention layer please see following article which is very easy and well explained\n",
    "# #https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e\n",
    "# class WordAttention(Layer):\n",
    "#     def __init__(self):\n",
    "#         super(WordAttention,self).__init__()\n",
    "#     def build(self,input_shapes):\n",
    "#         #we have one unit of MLP\n",
    "#         self.w=self.add_weight(shape=(1536,1), initializer=\"normal\")\n",
    "#         self.b=self.add_weight(shape=(512,1), initializer=\"zeros\")\n",
    "#         super(WordAttention, self).build(input_shapes)\n",
    "#     def call(self, x):\n",
    "#         #x is inp tensor of 1536 dimensions\n",
    "#         #main processing done during training\n",
    "#         e = K.tanh(K.dot(x,self.w)+self.b)\n",
    "#         a = K.softmax(e, axis=1)\n",
    "#         output = x*a\n",
    "        \n",
    "#         # return the outputs. 'a' is the set of 512 attention weights\n",
    "#         # the second variable is the 'attention adjusted o/p state'\n",
    "#         return a, K.sum(output, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################Runnig code###################\n",
    "# class WordAttention(Layer):\n",
    "#     def __init__(self):\n",
    "#         super(WordAttention,self).__init__()\n",
    "#     def build(self,input_shapes):\n",
    "#         #we have one unit of MLP\n",
    "#         self.w=self.add_weight(shape=(1536,1), initializer=\"normal\")\n",
    "#         self.b=self.add_weight(shape=(512,1), initializer=\"zeros\")\n",
    "#         super(WordAttention, self).build(input_shapes)\n",
    "#     def call(self, x):\n",
    "#         #x is inp of 1536 dimensions\n",
    "#         #main processing done during training\n",
    "#         e = K.tanh(K.dot(x,self.w)+self.b)\n",
    "#         a = K.softmax(e, axis=1)\n",
    "#         output = x*a\n",
    "        \n",
    "#         # return the outputs. 'a' is the set of 512 attention weights\n",
    "#         # the second variable is the 'attention adjusted o/p state'\n",
    "#         return a, K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U gast==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Source: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "            https://humboldt-wi.github.io/blog/research/information_systems_1819/group5_han/\n",
    "   \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,attention_dim=768,return_coefficients=False,**kwargs):\n",
    "        # Initializer\n",
    "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.attention_dim = attention_dim\n",
    "        self.supports_masking = True\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # print(input_shape[0])\n",
    "        # print(input_shape[-1])\n",
    "        # print(input_shape[1])\n",
    "        # Builds all weights\n",
    "        # u = context vector,W = Weight matrix, b = bias vector\n",
    "        assert len(input_shape) == 3\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
    "        self.W = K.variable(self.init((int(input_shape[-1]), self.attention_dim)),name='W')\n",
    "        \n",
    "        self._trainable_weights = [self.W, self.b, self.u]\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, hit, mask=None):\n",
    "        # Here, the actual calculation is done\n",
    "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "        \n",
    "        if mask is not None:\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = hit * ait\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return [K.sum(weighted_input, axis=1), ait]\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], int(input_shape[-1])), (input_shape[0], int(input_shape[-1]), 1)]\n",
    "        else:\n",
    "            return input_shape[0], int(input_shape[-1])\n",
    "\n",
    "    # def get_config(self):\n",
    "\n",
    "    #     config = super().get_config().copy()\n",
    "    #     config.update({\n",
    "    #         'attention_dim ': self.attention_dim ,\n",
    "    #         'return_coefficients': self.return_coefficients,\n",
    "    #     })\n",
    "    #     return config\n",
    "\n",
    "    def get_config(self):#to avoiding saving problem.check(https://stackoverflow.com/questions/50837728/valueerror-unknown-layer-capsulelayer and    \n",
    "        #https://stackoverflow.com/questions/58678836/notimplementederror-layers-with-arguments-in-init-must-override-get-conf)\n",
    "        # For serialization with 'custom_objects'\n",
    "        config = super().get_config()\n",
    "        config['attention_dim'] = self.attention_dim\n",
    "        config['return_coefficients'] = self.return_coefficients\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "# embedding_matrix = np.random.random((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "# max_sentenses=1\n",
    "# embedding_layer = Embedding(VOCAB_SIZE,EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH,trainable=False,weights=[embedding_matrix],name='word_embedding')\n",
    "MAX_SEQUENCE_LENGTH=512\n",
    "EMBEDDING_DIM=768\n",
    "max_sentenses=1\n",
    "\n",
    "# Words  attention\n",
    "emb_input = Input(shape=(MAX_SEQUENCE_LENGTH,EMBEDDING_DIM), dtype='float32',name='embeddings_input')\n",
    "# #word_sequences = embedding_layer(word_input)\n",
    "gru_word = Bidirectional(GRU(768, return_sequences=True),name='word_GRU')(emb_input)#by default if we set treturn_sequence=False it will\n",
    "#return only last words hidden state is returm only as the output having vector dimensions=2*GRU units \n",
    "#for bidirectional\n",
    "# word_dense = Dense(1536, activation='relu', name='word_dense')(gru_out) \n",
    "word_attention,word_coff= AttentionLayer(EMBEDDING_DIM,return_coefficients=True,name='word_attention')(gru_word)\n",
    "\n",
    "# dense_out = Dense(512,activation='relu', name='word_dense')(word_attention)#we may use different number of units\n",
    "# preds = Dense(13, activation='softmax',name='output')(dense_out)\n",
    "# model = Model(emb_input, preds)\n",
    "WordEnc = Model(inputs = emb_input,outputs = word_attention)#word encoder\n",
    "\n",
    "\n",
    "\n",
    "#Sentence Attention\n",
    "sentence_inp=Input(shape=(max_sentenses, MAX_SEQUENCE_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "sent_encoder = TimeDistributed(WordEnc,name='sent_encd')(sentence_inp)\n",
    "gru_sentense = Bidirectional(GRU(100, return_sequences=True),name='sentense_GRU')(sent_encoder)\n",
    "sentense_attention,sentense_coff= AttentionLayer(EMBEDDING_DIM,True,name='sentense_attention')(gru_sentense)\n",
    "dense_out = Dense(512,activation='relu', name='sentense_dense')(sentense_attention)#we may use different number of units\n",
    "dropout = Dropout(0.4,name='dropout')(dense_out)\n",
    "preds = Dense(13, activation='softmax',name='output')(dropout)\n",
    "model = Model(sentence_inp, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embeddings_input (InputLaye  [(None, 512, 768)]       0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " word_GRU (Bidirectional)    (None, 512, 1536)         7087104   \n",
      "                                                                 \n",
      " word_attention (AttentionLa  [(None, 1536),           1181184   \n",
      " yer)                         (None, 512, 1)]                    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,268,288\n",
      "Trainable params: 8,268,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 512, 768)]     0         \n",
      "                                                                 \n",
      " sent_encd (TimeDistributed)  (None, 1, 1536)          8268288   \n",
      "                                                                 \n",
      " sentense_GRU (Bidirectional  (None, 1, 200)           982800    \n",
      " )                                                               \n",
      "                                                                 \n",
      " sentense_attention (Attenti  [(None, 200),            155136    \n",
      " onLayer)                     (None, 1, 1)]                      \n",
      "                                                                 \n",
      " sentense_dense (Dense)      (None, 512)               102912    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 13)                6669      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,515,805\n",
      "Trainable params: 9,515,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "print(WordEnc.summary())\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1810, 512, 768)\n",
      "(1810,)\n",
      "(1810, 1, 512, 768)\n"
     ]
    }
   ],
   "source": [
    "#Adding dimension into training and test data and converting into numpy array\n",
    "# x_tr = np.array([x_train])\n",
    "# y_tr = np.array(y_train)\n",
    "x_ts=np.array([x_test])\n",
    "y_ts=np.array(y_test)\n",
    "#Reshaping inputs\n",
    "# print(x_tr.shape)\n",
    "# print(y_tr.shape)\n",
    "print(x_ts.shape)\n",
    "print(y_ts.shape)\n",
    "# x_trn=np.transpose(x_tr, (1, 0, 2,3))\n",
    "x_tst=np.transpose(x_ts, (1, 0, 2,3))\n",
    "# print(x_trn.shape)\n",
    "print(x_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import math\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    def __init__(self, x_train, y_train,batch_size):\n",
    "        self.x, self.y = x_train, y_train\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)#ValueError: axes don't match array\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     \n",
    "        x_tr = np.array([self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]])#we also added an extra dimension according to our requirement\n",
    "        y_tr = np.array(self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size])\n",
    "\n",
    "        x_trn=np.transpose(x_tr, (1, 0, 2,3))#reshape the input\n",
    "\n",
    "        return x_trn,y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=DataGenerator(x_train, y_train,32)\n",
    "val_generator=DataGenerator(x_val, y_val,32)\n",
    "#test_generator = DataGenerator(x_test,y_test,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: dict_keys([5, 2, 1, 7, 0, 11, 8, 3, 4, 12, 10, 9, 6])\n",
      "samples in each label: dict_values([969, 1438, 1045, 780, 588, 2363, 982, 1542, 491, 1312, 55, 72, 23])\n"
     ]
    }
   ],
   "source": [
    "#checking the total samples in each class(label)\n",
    "from collections import Counter\n",
    "print(f\"labels: {Counter(doc_label_list_arr).keys()}\")\n",
    "print(f\"samples in each label: {Counter(doc_label_list_arr).values()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making class weights dictionary (our classes are imbalance)\n",
    "#getting class having maximum samples\n",
    "class_samples={}\n",
    "train_text='/media/umar_visionx/Backup Plus/Active/Faizan/textrect_dataset/my_model.h5'\n",
    "for folder in os.listdir(train_text):\n",
    "    folder_path=os.path.join(train_text,folder)\n",
    "    class_samples[folder]=len(os.listdir(folder_path))\n",
    "print(f'max samples in class ( {max(class_samples, key=class_samples.get)}={class_samples[max(class_samples, key=class_samples.get)]})')\n",
    "max_samples=int(class_samples[max(class_samples, key=class_samples.get)])\n",
    "print('***********')\n",
    "#getting class weights\n",
    "weights={}\n",
    "for folder in os.listdir(train_text):\n",
    "    folder_path=os.path.join(train_text,folder)\n",
    "    weights[folder]=round(max_samples/int(len(os.listdir(folder_path))),2)\n",
    "print(f'class weights: {weights}')\n",
    "print(\"********\")\n",
    "#changing the names of keys in class weight dictionary\n",
    "class_weight={}\n",
    "for (keys1,values1),(keys2,values2) in zip(weights.items(),label_index.items()):\n",
    "\n",
    "    class_weight[int(values2)]=values1\n",
    "\n",
    "print(f'class weights: {class_weight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training\n",
    "checkpoint_path='/media/umar_visionx/Backup Plus/Active/Faizan/model_keras/my_model.h5'\n",
    "\n",
    "loss_list=[]\n",
    "#loss_list.append(history.history['val_loss'][0])\n",
    "history=model.fit(train_generator,validation_data=val_generator, epochs=1,class_weight=class_weight,shuffle=True)\n",
    "loss_list.append(history.history['val_loss'][0])\n",
    "for i in range(9):\n",
    "    history=model.fit(train_generator,validation_data=val_generator, epochs=1,class_weight=class_weight,shuffle=True)\n",
    "    loss=history.history['val_loss'][0]\n",
    "    if loss<loss_list[-1]:\n",
    "        #saving the model\n",
    "        model.save(checkpoint_path)\n",
    "        loss_list.append(history.history['val_loss'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #saving the model\n",
    "# model.save('/media/umar_visionx/Backup Plus/Active/Faizan/model_keras/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 15:03:36.486999: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'while' has 11 outputs but the _output_shapes attribute specifies shapes for 37 outputs. Output shapes may be inaccurate.\n",
      "2022-03-21 15:03:37.061754: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'while' has 11 outputs but the _output_shapes attribute specifies shapes for 37 outputs. Output shapes may be inaccurate.\n",
      "2022-03-21 15:03:37.073573: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'model/word_GRU/forward_gru/PartitionedCall' has 4 outputs but the _output_shapes attribute specifies shapes for 41 outputs. Output shapes may be inaccurate.\n",
      "2022-03-21 15:03:37.073811: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'model/word_GRU/backward_gru/PartitionedCall' has 4 outputs but the _output_shapes attribute specifies shapes for 42 outputs. Output shapes may be inaccurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 512, 768)]     0         \n",
      "                                                                 \n",
      " sent_encd (TimeDistributed)  (None, 1, 1536)          8268288   \n",
      "                                                                 \n",
      " sentense_GRU (Bidirectional  (None, 1, 200)           982800    \n",
      " )                                                               \n",
      "                                                                 \n",
      " sentense_attention (Attenti  [(None, 200),            155136    \n",
      " onLayer)                     (None, 1, 1)]                      \n",
      "                                                                 \n",
      " sentense_dense (Dense)      (None, 512)               102912    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 13)                6669      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,515,805\n",
      "Trainable params: 9,515,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('/media/umar_visionx/Backup Plus/Active/Faizan/model_keras/')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.09565596282482147],\n",
       " 'acc': [0.9690299034118652],\n",
       " 'val_loss': [0.44553402066230774],\n",
       " 'val_acc': [0.8936535120010376]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 83s 1s/step - loss: 1.4675 - acc: 0.7094\n",
      "Test loss: 1.4675407409667969\n",
      "Test accuracy: 0.709392249584198\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_tst, y_ts, verbose = 1) \n",
    "\n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ts[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0391946e-01, 1.6219815e-02, 1.3022219e-01, 7.7651935e-03,\n",
       "        1.2847678e-01, 3.9892595e-02, 9.1801547e-03, 3.1454569e-01,\n",
       "        7.9592057e-02, 4.2604249e-02, 4.8336651e-02, 5.1275589e-02,\n",
       "        2.7969463e-02],\n",
       "       [1.5764245e-01, 8.7256664e-03, 7.6922365e-02, 4.3933887e-02,\n",
       "        1.3934281e-01, 3.5013333e-02, 6.4677638e-03, 2.1565320e-01,\n",
       "        7.8749344e-02, 3.4783963e-02, 8.4994912e-02, 9.8145939e-02,\n",
       "        1.9624410e-02],\n",
       "       [1.4916790e-01, 5.2206498e-05, 1.7044856e-03, 8.1145996e-01,\n",
       "        5.2056927e-04, 3.2123746e-03, 9.3613904e-05, 1.6701395e-04,\n",
       "        1.5591676e-02, 6.3124549e-04, 2.0211812e-03, 1.4666222e-02,\n",
       "        7.1141985e-04],\n",
       "       [1.4672674e-01, 3.7247868e-05, 1.3714962e-03, 8.2115197e-01,\n",
       "        4.8205562e-04, 3.4428122e-03, 8.3032603e-05, 1.4420917e-04,\n",
       "        1.3295571e-02, 5.0027523e-04, 1.4545802e-03, 1.0700453e-02,\n",
       "        6.0947338e-04]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0 ... 12 12 12]\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(predictions, axis = 1)\n",
    "\n",
    "print(pred) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.59      0.67        83\n",
      "           1       0.62      0.71      0.66       336\n",
      "           2       0.65      0.73      0.69       369\n",
      "           3       0.93      0.96      0.94       179\n",
      "           4       0.87      0.54      0.67        63\n",
      "           5       0.54      0.23      0.32        31\n",
      "           6       0.00      0.00      0.00         8\n",
      "           7       0.80      0.83      0.82       181\n",
      "           8       0.62      0.51      0.56       266\n",
      "           9       0.79      0.54      0.64        41\n",
      "          10       1.00      0.22      0.36         9\n",
      "          11       0.75      0.93      0.83       169\n",
      "          12       0.95      0.84      0.89        75\n",
      "\n",
      "    accuracy                           0.72      1810\n",
      "   macro avg       0.71      0.59      0.62      1810\n",
      "weighted avg       0.72      0.72      0.71      1810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_ts,pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "229bb8c562dc6514b703716efac498ecdfe3964b4336041201b73c82b78cece5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
